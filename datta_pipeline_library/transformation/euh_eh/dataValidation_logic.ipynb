{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "003d0559-da7a-4f0e-9711-9277dfb4bea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./common_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7dad268-2709-481c-b6db-daac34c94d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim,regexp_replace,current_date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "table_name=\"dataValidation_Output_table\" \n",
    "path = eh_folder_path+\"/\"+table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "796c552d-bdf7-417f-8172-ff4f17bb14f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df = spark.read.format(\"delta\").table(\"`cross_ds-unitycatalog-dev`.`raw-ds-gsap`.dataValidation_Control_table\")\n",
    "\n",
    "\"\"\"\n",
    "Data validation control table\n",
    "return: Dataframe\n",
    "\"\"\"\n",
    "df_DV = spark.read.table(f\"`{uc_catalog_name}`.`{uc_raw_schema}`.dataValidation_Control_table\")\n",
    "spark.sql(f\"Delete from `{uc_catalog_name}`.`{uc_eh_schema}`.dataValidation_Output_table\")\n",
    "\n",
    "for row in df_DV.collect():\n",
    "    table_name = row[\"Table_name\"]\n",
    "    measure = row[\"Measure\"]\n",
    "    date_field = row[\"Date_field\"]\n",
    "    From_Ind = row[\"From_Ind\"]\n",
    "    To_Ind = row[\"To_Ind\"]\n",
    "\n",
    "    query = f\"\"\"\n",
    "            select \n",
    "                '{table_name}' AS Table_name,\n",
    "                count(*) AS Record_Count,\n",
    "                '{measure}' AS Measure,\n",
    "                sum({measure}) AS Measure_Sum,\n",
    "                '{From_Ind}',\n",
    "                '{To_Ind}'\n",
    "            from \n",
    "                 `{uc_catalog_name}`.`{uc_euh_schema}`.{table_name}\n",
    "            where\n",
    "                {date_field} > current_date() - {From_Ind} AND {date_field} < current_date() - {To_Ind}\n",
    "    \"\"\"\n",
    "\n",
    "    df_output = spark.sql(query);\n",
    "    df_output.withColumn('Measure',regexp_replace('Measure',\"`\",\"\")).withColumn('ingested_at',current_date()).write.mode('append').insertInto(f\"`{uc_catalog_name}`.`{uc_eh_schema}`.dataValidation_Output_table\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dataValidation_logic",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
