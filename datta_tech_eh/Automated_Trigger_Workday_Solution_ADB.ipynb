{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6cab69c-98cf-468c-aa88-6e3730743bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install msal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb1a879-9289-4136-ad0a-2ca08fc61b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install adal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc0fc43-41e7-4414-849a-4062c2061a8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0584dc21-0f02-4e08-b085-386d99ab3b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, time, timedelta, date\n",
    "import itertools\n",
    "from pyspark.sql.functions import last_day\n",
    "from datta_pipeline_library.core.base_config import (\n",
    "    BaseConfig,\n",
    "    CollibraConfig,\n",
    "    CommonConfig,\n",
    "    EnvConfig\n",
    ")\n",
    "from datta_pipeline_library.helpers.adls import configure_spark_to_use_spn_to_write_to_adls_gen2\n",
    "from datta_pipeline_library.helpers.spn import AzureSPN\n",
    "# from datta_pipeline_library.edc.collibra import fetch_business_metadata\n",
    "from datta_pipeline_library.helpers.uc import (\n",
    "    get_catalog_name,\n",
    "    get_raw_schema_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a9a352-087f-4a3b-a512-47cfda46253f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "env = dbutils.widgets.get(name=\"env\")\n",
    "repos_path = dbutils.widgets.get(name=\"Tech_EH_Repo_Path\")\n",
    "unique_repo_branch_id = dbutils.widgets.get(name=\"unique_repo_branch_id\")\n",
    "unique_repo_branch_id_schema = dbutils.widgets.get(name=\"unique_repo_branch_id_schema\")\n",
    "unique_repo_branch_id = unique_repo_branch_id.replace(\"datta\",\"\")\n",
    "unique_repo_branch_id_schema = unique_repo_branch_id_schema.replace(\"datta\",\"\")\n",
    "print(\"unique_repo_branch_id:\",unique_repo_branch_id)\n",
    "print(\"unique_repo_branch_id_schema:\",unique_repo_branch_id_schema)\n",
    "\n",
    "# env = \"dev\"\n",
    "# repos_path = \"/Repos/DATTA-AUTO_TRIGGER_CAPABILITY/DATTA-TECH-EH\"\n",
    "# unique_repo_branch_id = \"\"\n",
    "# unique_repo_branch_id_schema = \"\"\n",
    "\n",
    "common_conf = CommonConfig.from_file(f\"/Workspace/{repos_path.strip('/')}/conf/common/common_conf.json\")\n",
    "print(\"common_conf !!!\")\n",
    "env_conf = EnvConfig.from_file(f\"/Workspace/{repos_path.strip('/')}/conf/{env}/conf.json\")\n",
    "\n",
    "base_config = BaseConfig.from_confs(env_conf, common_conf)\n",
    "if unique_repo_branch_id:\n",
    "    base_config.set_unique_id(unique_repo_branch_id)\n",
    "if unique_repo_branch_id_schema:\n",
    "    base_config.set_unique_id_schema(unique_repo_branch_id_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093e2fae-dfe5-4a3b-bb38-e07e1dd43b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uc_catalog_name = base_config.get_uc_catalog_name()\n",
    "print(\"uc_catalog_name : \",uc_catalog_name)\n",
    "uc_raw_schema = base_config.get_uc_raw_schema()\n",
    "print(\"uc_raw_schema : \",uc_raw_schema)\n",
    "\n",
    "raw_folder_path = base_config.get_raw_folder_path()\n",
    "print(\"raw_folder_path : \",raw_folder_path)\n",
    "\n",
    "tbl_owner_grp = base_config.get_tbl_owner_grp()\n",
    "print(\"tbl_owner_grp : \",tbl_owner_grp)\n",
    "tbl_read_grp = base_config.get_tbl_read_grp()\n",
    "print(\"tbl_read_grp : \",tbl_owner_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f6616fb-fdf2-417a-9163-7f20c92187a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_hours = 1   #int, \n",
    "starttime = spark.sql(f\"select cast(concat(date_format(dateadd(HOUR, 1, current_timestamp()),'HH'),':00:00') as timestamp)\").first()[0] #timestamp\n",
    "endtime  = spark.sql(f\"select cast(concat(date_format(dateadd(HOUR, 2, current_timestamp()),'HH'),':00:00') as timestamp)\").first()[0] #timestamp\n",
    "\n",
    "ist_starttime = spark.sql(f\"SELECT cast(concat(date_format(dateadd(HOUR, 1, from_utc_timestamp(current_timestamp(),'GMT+5:30')),'HH'),':00:00') as timestamp)\").first()[0]\n",
    "ist_endtime = spark.sql(f\"SELECT cast(concat(date_format(dateadd(HOUR, 2, from_utc_timestamp(current_timestamp(),'GMT+5:30')),'HH'),':00:00') as timestamp)\").first()[0]\n",
    "\n",
    "if endtime == datetime.combine(date.today(),datetime.strptime('000000','%H%M%S').time()):\n",
    "    endtime = datetime.combine(date.today(),datetime.strptime('235900','%H%M%S').time())\n",
    "if ist_endtime == datetime.combine(date.today(),datetime.strptime('000000','%H%M%S').time()):\n",
    "    ist_endtime = datetime.combine(date.today(),datetime.strptime('235900','%H%M%S').time())\n",
    "\n",
    "print(starttime)\n",
    "print(endtime)\n",
    "print(ist_starttime)\n",
    "print(ist_endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8709be-53ac-472c-8f5c-5ce93b86865a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "__15_minutes_list = []\n",
    "__30_minutes_list = []\n",
    "__45_minutes_list = []\n",
    "__60_minutes_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba263d0-42c2-4ab3-ac4d-b6c06437fcd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def doset(k_list, map=map, list=list, set=set, tuple=tuple):\n",
    "  return map(list, set(map(tuple, k_list)))\n",
    "\n",
    "def recursive_auto_schedule_15_mins(end_time_15):\n",
    "    if end_time_15 < endtime and endtime != datetime.combine(date.today(),datetime.strptime('000000','%H%M%S').time()):\n",
    "        __15_minutes_list.append([end_time_15,(end_time_15 + timedelta(minutes=15)), float('15')])\n",
    "        return recursive_auto_schedule_15_mins(end_time_15 + timedelta(minutes=15))\n",
    "    else:\n",
    "        return __15_minutes_list\n",
    "\n",
    "def recursive_auto_schedule_30_mins(end_time_30):\n",
    "    if end_time_30 < endtime and endtime != datetime.combine(date.today(),datetime.strptime('000000','%H%M%S').time()):\n",
    "        __30_minutes_list.append([end_time_30,(end_time_30 + timedelta(minutes=30)), float('30')])\n",
    "        return recursive_auto_schedule_30_mins(end_time_30 + timedelta(minutes=30))\n",
    "    else:\n",
    "        return __30_minutes_list\n",
    "    \n",
    "def recursive_auto_schedule_45_mins(end_time_45):\n",
    "    if end_time_45 < endtime and endtime != datetime.combine(date.today(),datetime.strptime('003000','%H%M%S').time()):\n",
    "        __45_minutes_list.append([end_time_45,(end_time_45 + timedelta(minutes=45)), float('45')])\n",
    "        return recursive_auto_schedule_45_mins(end_time_45 + timedelta(minutes=45))\n",
    "    else:\n",
    "        return __45_minutes_list\n",
    "    \n",
    "def recursive_auto_schedule_60_mins(end_time_60):\n",
    "    if end_time_60 < endtime and endtime != datetime.combine(date.today(),datetime.strptime('000000','%H%M%S').time()):\n",
    "        __60_minutes_list.append([end_time_60,(end_time_60 + timedelta(minutes=60)), float('60')])\n",
    "        return recursive_auto_schedule_60_mins(end_time_60 + timedelta(minutes=60))\n",
    "    else:\n",
    "        return __60_minutes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91975014-bec9-4658-9abb-777fd7c2cbf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "__15_minutes_list_1 = recursive_auto_schedule_15_mins(starttime)\n",
    "__15_minutes_final_list = doset(__15_minutes_list_1)\n",
    "cols = ['starttime', 'endtime', 'duration']\n",
    "df_15_minutes = spark.createDataFrame(__15_minutes_final_list).toDF(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd14a3d9-e347-47c8-9b16-bc854bb9952d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "__30_minutes_list_1 = recursive_auto_schedule_30_mins(starttime)\n",
    "__30_minutes_final_list = doset(__30_minutes_list_1)\n",
    "cols = ['starttime', 'endtime', 'duration']\n",
    "df_30_minutes = spark.createDataFrame(__30_minutes_final_list).toDF(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666d353b-6bbe-497a-be14-c3321a90a98d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "__45_minutes_list_1 = recursive_auto_schedule_45_mins(starttime)\n",
    "__45_minutes_final_list = doset(__45_minutes_list_1)\n",
    "cols = ['starttime', 'endtime', 'duration']\n",
    "df_45_minutes = spark.createDataFrame(__45_minutes_final_list).toDF(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d068ba4c-fa27-4ef4-a7be-f578ee18d34f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "__60_minutes_list_1 = recursive_auto_schedule_60_mins(starttime)\n",
    "__60_minutes_final_list = doset(__60_minutes_list_1)\n",
    "cols = ['starttime', 'endtime', 'duration']\n",
    "df_60_minutes = spark.createDataFrame(__60_minutes_final_list).toDF(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491a3eb0-b571-48a0-84a4-3f9f728514be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tempHourlyfrequency = df_15_minutes.union(df_30_minutes).union(df_45_minutes).union(df_60_minutes)\n",
    "df_tempHourlyfrequency.createOrReplaceTempView(\"tempHourlyfrequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd46a654-4064-418e-a372-3af826aa4066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gettig trigger and wait time from trigger frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7e066f-8c43-446a-bbfb-7dda5a0a3b13",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Minute wise triggers"
    }
   },
   "outputs": [],
   "source": [
    "df_minutes_wise_triggers = spark.sql(f\"\"\"SELECT tr.PipelineName,tr.ParameterName1,tr.ParameterValue1,\n",
    "                                    tr.ParameterName2,tr.ParameterValue2,tr.ParameterName3,tr.ParameterValue3,\n",
    "                                    dateadd(minute,try_cast(tr.`Minute` AS INT),c.starttime) AS trigertime\n",
    "                                    ,cast(current_timestamp() as timestamp) as CurrentTime\n",
    "                                    ,CASE WHEN SUBSTRING(CAST(c.starttime AS string),12,2)='00' THEN \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat('23',':','59') as timestamp))+60+(60*try_cast(tr.`Minute` as INT))+(60*cast((SUBSTRING(CAST(c.starttime AS string),15,2)) AS INT))\n",
    "                                    ELSE \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),dateadd(minute,try_cast(tr.`Minute` as INT),c.starttime)) \n",
    "                                    END as WaitTime\n",
    "                                    ,tr.ADFType\n",
    "                                    ,tr.JobID\n",
    "                                from `{uc_catalog_name}`.`{uc_raw_schema}`.tb_trigger_frequency tr\n",
    "                                CROSS JOIN tempHourlyfrequency  c\n",
    "                                where tr.Frequency='Minutes' \n",
    "                                    AND tr.`Duration`=cast(c.`Duration` AS FLOAT)\n",
    "                                    AND tr.ActiveFlag='Y'\n",
    "                                    AND coalesce(tr.TriggerEndDateTime, '3000-12-31 00:00:00.000') > current_timestamp()\n",
    "                                    AND tr.ADFType='IngestionADB'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd832f4f-c363-45c3-875e-57f73b5d19ed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hourly Triggers like 1 hourly, 2 hourly, 3 hourly"
    }
   },
   "outputs": [],
   "source": [
    "df_hourly_wise_triggers = spark.sql(f\"\"\"SELECT PipelineName,ParameterName1,ParameterValue1,\n",
    "                                    ParameterName2,ParameterValue2,ParameterName3,ParameterValue3,\n",
    "                                    cast(concat(DATE_FORMAT(DATEADD(HOUR,1,current_timestamp()),'HH'),':',`Minute`) as timestamp) AS TrigerTime\n",
    "                                    ,cast(current_timestamp() as timestamp) as CurrentTime\n",
    "                                    ,CASE WHEN DATE_FORMAT(DATEADD(HOUR,1,current_timestamp()),'HH')='00' THEN \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat('23',':','59') as timestamp))+60+(60*try_cast(`Minute` as INT))\n",
    "                                    ELSE \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat(DATE_FORMAT(DATEADD(HOUR,1,current_timestamp()),'HH'),':',`Minute`) as timestamp)) \n",
    "                                    END as WaitTime\n",
    "                                    ,ADFType\n",
    "                                    ,JobID\n",
    "                                from `{uc_catalog_name}`.`{uc_raw_schema}`.tb_trigger_frequency\n",
    "                                where Frequency='Hourly' \n",
    "                                    AND coalesce(TriggerEndDateTime, '3000-12-31 00:00:00.000') > current_timestamp()\n",
    "                                    AND ((DATE_FORMAT(DATEADD(HOUR,1,current_timestamp()),'HH') - cast(`Hour` as int )) % cast(`Duration` as int )) = 0\n",
    "                                    AND ActiveFlag='Y'\n",
    "                                    AND (HOUR(current_timestamp()) between coalesce(`IntervalStart`, 0) and coalesce(`IntervalEnd`, 23))\n",
    "                                    AND ADFType='IngestionADB'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "858f96c4-e0b8-4062-89d7-88ae6e924271",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Regular interval triggers days like daily , 2days, 3days"
    }
   },
   "outputs": [],
   "source": [
    "df_regular_interval_triggers = spark.sql(f\"\"\"SELECT PipelineName,ParameterName1,ParameterValue1,\n",
    "                                    ParameterName2,ParameterValue2,ParameterName3,ParameterValue3,\n",
    "                                    cast(concat(`Hour`,':',`Minute`) as timestamp) AS TrigerTime\n",
    "                                    ,cast(current_timestamp() as timestamp) as CurrentTime\n",
    "                                    ,CASE WHEN `Hour`='00' THEN \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat('23',':','59') as timestamp))+60+(60*try_cast(`Minute` as INT))\n",
    "                                    ELSE \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat(`Hour`,':',`Minute`) as timestamp)) \n",
    "                                    END as WaitTime\n",
    "                                    ,ADFType\n",
    "                                    ,JobID\n",
    "                                from `{uc_catalog_name}`.`{uc_raw_schema}`.tb_trigger_frequency\n",
    "                                where Frequency='Daily' \n",
    "                                    AND coalesce(TriggerEndDateTime, '3000-12-31 00:00:00.000') > current_timestamp()\n",
    "                                    AND DATEDIFF(day,TriggerStartDateTime,DATEADD(HOUR,1,current_timestamp())) % cast(`Duration` as int) = 0\n",
    "                                    AND `Hour`=CAST(DATE_FORMAT(DATEADD(HOUR,1,current_timestamp()),'HH') AS INT)\n",
    "                                    AND ActiveFlag='Y'\n",
    "                                    AND (DAY(current_timestamp()) between coalesce(IntervalStart, 1) and coalesce(IntervalEnd, 31))\n",
    "                                    AND ADFType='IngestionADB'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08962681-ab0c-439c-a1c3-52e5b2051e58",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Daily_workday trigger to run daily except weekends"
    }
   },
   "outputs": [],
   "source": [
    "df_daily_workday_interval_triggers = spark.sql(f\"\"\"SELECT PipelineName,ParameterName1,ParameterValue1,\n",
    "                                    ParameterName2,ParameterValue2,ParameterName3,ParameterValue3,\n",
    "                                    cast(concat(`Hour`,':',`Minute`) as timestamp) AS TrigerTime\n",
    "                                    ,cast(current_timestamp() as timestamp) as CurrentTime\n",
    "                                    ,CASE WHEN `Hour`='00' THEN \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat('23',':','59') as timestamp))+60+(60*try_cast(`Minute` as INT))\n",
    "                                    ELSE \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat(`Hour`,':',`Minute`) as timestamp)) \n",
    "                                    END as WaitTime\n",
    "                                    ,ADFType\n",
    "                                    ,JobID\n",
    "                                from `{uc_catalog_name}`.`{uc_raw_schema}`.tb_trigger_frequency\n",
    "                                where Frequency='Daily_Workday' \n",
    "                                    AND coalesce(TriggerEndDateTime, '3000-12-31 00:00:00.000') > current_timestamp()\n",
    "                                    AND DATEDIFF(day,TriggerStartDateTime,DATEADD(HOUR,1,current_timestamp())) % cast(`Duration` as int) = 0\n",
    "                                    AND `Hour`=CAST(DATE_FORMAT(DATEADD(HOUR,1,current_timestamp()),'HH') AS INT)\n",
    "                                    AND ActiveFlag='Y'\n",
    "                                    AND (DAY(current_timestamp()) between coalesce(IntervalStart, 1) and coalesce(IntervalEnd, 31))\n",
    "                                    AND date_format(current_timestamp(), 'EEEE') NOT IN ('Saturday','Sunday')\n",
    "                                    AND ADFType='IngestionADB'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33d122c-68f9-4019-a312-8376686b4d78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Weekly Triggers"
    }
   },
   "outputs": [],
   "source": [
    "df_weekly_triggers = spark.sql(f\"\"\"SELECT PipelineName,ParameterName1,ParameterValue1,\n",
    "                                    ParameterName2,ParameterValue2,ParameterName3,ParameterValue3,\n",
    "                                    cast(concat(`Hour`,':',`Minute`) as timestamp) AS TrigerTime\n",
    "                                    ,cast(current_timestamp() as timestamp) as CurrentTime\n",
    "                                    ,CASE WHEN `Hour`='00' THEN \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat('23',':','59') as timestamp))+60+(60*try_cast(`Minute` as INT))\n",
    "                                    ELSE \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat(`Hour`,':',`Minute`) as timestamp)) \n",
    "                                    END as WaitTime\n",
    "                                    ,ADFType\n",
    "                                    ,JobID\n",
    "                                from `{uc_catalog_name}`.`{uc_raw_schema}`.tb_trigger_frequency\n",
    "                                where Frequency='Weekly' \n",
    "                                    AND coalesce(TriggerEndDateTime, '3000-12-31 00:00:00.000') > current_timestamp()\n",
    "                                    AND dayOfWeek = (CASE WHEN 'Hour'='00' THEN date_format(dateadd(day,1,current_timestamp()), 'EEEE') ELSE date_format(current_timestamp(), 'EEEE') END ) \n",
    "                                    AND `Hour`=CAST(DATE_FORMAT(DATEADD(HOUR,1,current_timestamp()),'HH') AS INT)\n",
    "                                    AND ActiveFlag='Y'\n",
    "                                    AND ADFType='IngestionADB'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e7af9d-9ff6-4268-bca8-707a08226fdb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Monthly triggers"
    }
   },
   "outputs": [],
   "source": [
    "df_monthly_triggers = spark.sql(f\"\"\"SELECT PipelineName,ParameterName1,ParameterValue1,\n",
    "                                    ParameterName2,ParameterValue2,ParameterName3,ParameterValue3,\n",
    "                                    cast(concat(`Hour`,':',`Minute`) as timestamp) AS TrigerTime\n",
    "                                    ,cast(current_timestamp() as timestamp) as CurrentTime\n",
    "                                    ,CASE WHEN `Hour`='00' THEN \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat('23',':','59') as timestamp))+60+(60*try_cast(`Minute` as INT))\n",
    "                                    ELSE \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat(`Hour`,':',`Minute`) as timestamp)) \n",
    "                                    END as WaitTime\n",
    "                                    ,ADFType\n",
    "                                    ,JobID\n",
    "                                from `{uc_catalog_name}`.`{uc_raw_schema}`.tb_trigger_frequency\n",
    "                                where Frequency='Monthly' \n",
    "                                    AND coalesce(TriggerEndDateTime, '3000-12-31 00:00:00.000') > current_timestamp()\n",
    "                                    AND (\n",
    "                                        (cast(`Duration` as int) = 0  AND (replace(ltrim(replace(dayOfMonth,'0',' ')),' ','0')=CASE WHEN date_format(dateadd(day,1, current_timestamp()),'yyyy-MM-dd') = last_day(current_timestamp()) AND `Hour`='00' then '-1' \n",
    "                                        WHEN date_format(current_timestamp(),'yyyy-MM-dd') = last_day(current_timestamp()) THEN '-1' \n",
    "                                        WHEN `Hour`='00' THEN day(dateadd(day,1,current_timestamp())) \n",
    "                                        ELSE day(current_timestamp()) END ) \n",
    "                                        )\n",
    "                                        --Monthly specific week and day, like 2nd Saturday, 3rd Monday.\n",
    "                                        OR\n",
    "                                        (cast(`Duration` as int) > 0 \n",
    "                                        AND `dayOfWeek` = (\n",
    "                                            CASE WHEN `Hour`='00' THEN (\n",
    "                                            CASE WHEN ((dayofweek(dateadd(day,1,current_timestamp())) + 7 - 2) % 7) = 0 THEN 'Monday'\n",
    "                                                WHEN ((dayofweek(dateadd(day,1,current_timestamp())) + 7 - 2) % 7) = 1 THEN 'Tuesday'\n",
    "                                                WHEN ((dayofweek(dateadd(day,1,current_timestamp())) + 7 - 2) % 7) = 2 THEN 'Wednesday'\n",
    "                                                WHEN ((dayofweek(dateadd(day,1,current_timestamp())) + 7 - 2) % 7) = 3 THEN 'Thursday'\n",
    "                                                WHEN ((dayofweek(dateadd(day,1,current_timestamp())) + 7 - 2) % 7) = 4 THEN 'Friday'\n",
    "                                                WHEN ((dayofweek(dateadd(day,1,current_timestamp())) + 7 - 2) % 7) = 5 THEN 'Saturday'\n",
    "                                                WHEN ((dayofweek(dateadd(day,1,current_timestamp())) + 7 - 2) % 7) = 6 THEN 'Sunday'\n",
    "                                                ELSE 'Invalid Day' END\n",
    "                                              )\n",
    "                                            ELSE (\n",
    "                                                CASE WHEN ((dayofweek(current_timestamp()) + 7 - 2) % 7) = 0 THEN 'Monday'\n",
    "                                                    WHEN ((dayofweek(current_timestamp()) + 7 - 2) % 7) = 1 THEN 'Tuesday'\n",
    "                                                    WHEN ((dayofweek(current_timestamp()) + 7 - 2) % 7) = 2 THEN 'Wednesday'\n",
    "                                                    WHEN ((dayofweek(current_timestamp()) + 7 - 2) % 7) = 3 THEN 'Thursday'\n",
    "                                                    WHEN ((dayofweek(current_timestamp()) + 7 - 2) % 7) = 4 THEN 'Friday'\n",
    "                                                    WHEN ((dayofweek(current_timestamp()) + 7 - 2) % 7) = 5 THEN 'Saturday'\n",
    "                                                    WHEN ((dayofweek(current_timestamp()) + 7 - 2) % 7) = 6 THEN 'Sunday'\n",
    "                                                    ELSE 'Invalid Day' END\n",
    "                                            ) END \n",
    "                                            ) \n",
    "                                        AND cast((day(current_timestamp())-1)/7 AS INT)+1 = cast(`Duration` as int)\n",
    "                                        )\n",
    "                                    )\n",
    "                                    AND `Hour`=CAST(DATE_FORMAT(DATEADD(HOUR,1,current_timestamp()),'HH') AS INT)\n",
    "                                    AND (month(current_timestamp())) BETWEEN coalesce(IntervalStart, 1) AND coalesce(IntervalEnd, 12)\n",
    "                                    AND ActiveFlag='Y'\n",
    "                                    AND ADFType='IngestionADB'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7708f07d-ecad-40f3-95fa-9111171ce8c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Monthly_Workday triggers"
    }
   },
   "outputs": [],
   "source": [
    "df_monthly_workday_triggers = spark.sql(f\"\"\"SELECT PipelineName,ParameterName1,ParameterValue1,\n",
    "                                    ParameterName2,ParameterValue2,ParameterName3,ParameterValue3,\n",
    "                                    cast(concat(`Hour`,':',`Minute`) as timestamp) AS TrigerTime\n",
    "                                    ,cast(current_timestamp() as timestamp) as CurrentTime\n",
    "                                    ,CASE WHEN `Hour`='00' THEN \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat('23',':','59') as timestamp))+60+(60*try_cast(`Minute` as INT))\n",
    "                                    ELSE \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat(`Hour`,':',`Minute`) as timestamp)) \n",
    "                                    END as WaitTime\n",
    "                                    ,ADFType\n",
    "                                    ,JobID\n",
    "                                from `{uc_catalog_name}`.`{uc_raw_schema}`.tb_trigger_frequency\n",
    "                                where Frequency='Monthly_Workday' \n",
    "                                    AND coalesce(TriggerEndDateTime, '3000-12-31 00:00:00.000') > current_timestamp()\n",
    "                                    AND (replace(ltrim(replace(dayOfMonth,'0',' ')),' ','0') = CASE WHEN `Hour`='00' THEN ((DATEDIFF(day, DATEADD(month, DATEDIFF(month, CAST(0 AS timestamp), dateadd(day,1,current_timestamp())), CAST(0 AS timestamp)), dateadd(day,1,current_timestamp())) + 1) - (CAST(round(DATEDIFF(DAY, DATEADD(MONTH, DATEDIFF(MONTH, CAST(0 AS timestamp), dateadd(day,1,current_timestamp())), CAST(0 AS timestamp)), dateadd(day,1,current_timestamp()))/7,1) AS INT) * 2) - (CASE WHEN date_format(DATEADD(MONTH, DATEDIFF(MONTH, CAST(0 AS timestamp), dateadd(day,1,current_timestamp())), CAST(0 AS timestamp)), 'EEEE') = 'Sunday' THEN 1 ELSE 0 END) - (CASE WHEN date_format(dateadd(day,1,current_timestamp()), 'EEEE') = 'Saturday' THEN 1 ELSE 0 END)) \n",
    "                                    ELSE ((DATEDIFF(day, DATEADD(month, DATEDIFF(month, CAST(0 AS timestamp), current_timestamp()), CAST(0 AS timestamp)), current_timestamp()) + 1) - (CAST(round(DATEDIFF(DAY, DATEADD(MONTH, DATEDIFF(MONTH, CAST(0 AS timestamp), current_timestamp()), CAST(0 AS timestamp)), current_timestamp())/7,1) AS INT) * 2) - (CASE WHEN date_format(DATEADD(MONTH, DATEDIFF(MONTH, CAST(0 AS timestamp), current_timestamp()), CAST(0 AS timestamp)), 'EEEE') = 'Sunday' THEN 1 ELSE 0 END) - (CASE WHEN date_format(current_timestamp(), 'EEEE') = 'Saturday' THEN 1 ELSE 0 END)) END\n",
    "                                    )\n",
    "                                    AND `Hour`=CAST(DATE_FORMAT(DATEADD(HOUR,1,current_timestamp()),'HH') AS INT)\n",
    "                                    AND (month(current_timestamp()) BETWEEN coalesce(IntervalStart, 1) AND coalesce(IntervalEnd, 12))\n",
    "                                    AND ActiveFlag='Y'\n",
    "                                    AND ADFType='IngestionADB'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "463c8000-ba12-422f-9692-ae2011172829",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quarterly triggers"
    }
   },
   "outputs": [],
   "source": [
    "df_quarterly_triggers = spark.sql(f\"\"\"SELECT PipelineName,ParameterName1,ParameterValue1,\n",
    "                                    ParameterName2,ParameterValue2,ParameterName3,ParameterValue3,\n",
    "                                    cast(concat(`Hour`,':',`Minute`) as timestamp) AS TrigerTime\n",
    "                                    ,cast(current_timestamp() as timestamp) as CurrentTime\n",
    "                                    ,CASE WHEN `Hour`='00' THEN \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat('23',':','59') as timestamp))+60+(60*try_cast(`Minute` as INT))\n",
    "                                    ELSE \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat(`Hour`,':',`Minute`) as timestamp)) \n",
    "                                    END as WaitTime\n",
    "                                    ,ADFType\n",
    "                                    ,JobID\n",
    "                                FROM `{uc_catalog_name}`.`{uc_raw_schema}`.tb_trigger_frequency\n",
    "                                WHERE Frequency='Quarterly' \n",
    "                                AND coalesce(TriggerEndDateTime, '3000-12-31 00:00:00.000') > current_timestamp()\n",
    "                                AND ((month(current_timestamp()) in (1,4,7,10)) OR (month(current_timestamp()) in (12,3,6,9) AND CAST(DATE_FORMAT(current_timestamp(),'HH') AS INT)=23))\n",
    "                                AND (replace(ltrim(replace(dayOfMonth,'0',' ')),' ','0')=CASE WHEN `Hour`='00' THEN DAY(dateadd(day,1,current_timestamp())) ELSE DAY(current_timestamp()) END ) \n",
    "                                AND `Hour`=CAST(DATE_FORMAT(DATEADD(HOUR,1,current_timestamp()),'HH') AS INT)\n",
    "                                AND ActiveFlag='Y' \n",
    "                                AND ADFType='IngestionADB'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "206de790-45a5-43ad-bfc5-f5b92c419be9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Yearly triggers"
    }
   },
   "outputs": [],
   "source": [
    "df_yearly_triggers = spark.sql(f\"\"\"SELECT PipelineName,ParameterName1,ParameterValue1,\n",
    "                                    ParameterName2,ParameterValue2,ParameterName3,ParameterValue3,\n",
    "                                    cast(concat(`Hour`,':',`Minute`) as timestamp) AS TrigerTime\n",
    "                                    ,cast(current_timestamp() as timestamp) as CurrentTime\n",
    "                                    ,CASE WHEN `Hour`='00' THEN \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat('23',':','59') as timestamp))+60+(60*try_cast(`Minute` as INT))\n",
    "                                    ELSE \n",
    "                                        datediff(second,cast(current_timestamp() as timestamp),cast(concat(`Hour`,':',`Minute`) as timestamp)) \n",
    "                                    END as WaitTime\n",
    "                                    ,ADFType\n",
    "                                    ,JobID\n",
    "                                FROM `{uc_catalog_name}`.`{uc_raw_schema}`.tb_trigger_frequency\n",
    "                                WHERE Frequency='Yearly' \n",
    "                                AND coalesce(TriggerEndDateTime, '3000-12-31 00:00:00.000') > current_timestamp()\n",
    "                                AND (replace(ltrim(replace(monthOfYear,'0',' ')),' ','0')=month(current_timestamp()) OR (CAST(DATE_FORMAT(current_timestamp(),'HH') AS INT)=23 and replace(ltrim(replace(monthOfYear,'0',' ')),' ','0') = month(current_timestamp())+1))\n",
    "                                AND (replace(ltrim(replace(dayOfMonth,'0',' ')),' ','0')=CASE WHEN `Hour`='00' THEN DAY(dateadd(day,1,current_timestamp())) ELSE DAY(current_timestamp()) END )\n",
    "                                AND `Hour`=CAST(DATE_FORMAT(DATEADD(HOUR,1,current_timestamp()),'HH') AS INT)\n",
    "                                AND ActiveFlag='Y' \n",
    "                                AND ADFType='IngestionADB'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123657a5-3f03-44d6-ab55-94f53d97c09e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Total_Triggers = df_minutes_wise_triggers.union(df_hourly_wise_triggers).union(df_regular_interval_triggers).union(df_daily_workday_interval_triggers).union(df_weekly_triggers).union(df_monthly_triggers).union(df_monthly_workday_triggers).union(df_quarterly_triggers).union(df_yearly_triggers)\n",
    "\n",
    "Total_Triggers_pandas = Total_Triggers.toPandas()\n",
    "Total_Triggers_Json = Total_Triggers_pandas.to_json(orient='records', date_format='iso',date_unit='ms')\n",
    "print(Total_Triggers_Json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8def45f-7128-4979-bf44-be25a509f535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(Total_Triggers_Json)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5488988661167358,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Automated_Trigger_Workday_Solution_ADB",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
